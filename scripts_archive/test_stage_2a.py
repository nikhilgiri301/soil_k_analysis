#!/usr/bin/env python3
"""
Test Stage 2A: Soil K Specific Extraction Testing
Extracts soil potassium specific information and parameters in isolation
"""

import asyncio
import sys
import os
import json
from datetime import datetime

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import stage test framework
from stage_test_utils import StageTestFramework, calculate_stage_cost, validate_paper_exists

# Add synthesis engine to path
sys.path.append(os.path.join(os.path.dirname(__file__), '6_synthesis_engine'))

# Import necessary synthesis engine components
from utils.gemini_client import GeminiClient
from utils.prompt_loader import PromptLoader
from stage_2_processors.soilk_extractor import SoilKExtractor

async def test_stage_2a(paper_filename: str, api_key: str, disable_cache: bool = False, 
                       verbose: bool = False, save_debug: bool = False, 
                       use_main_cache: bool = False):
    """
    Test Stage 2A: Soil K Specific Extraction in isolation
    
    Args:
        paper_filename: Name of paper to test
        api_key: Gemini API key
        disable_cache: Whether to bypass cache
        verbose: Show detailed output
        save_debug: Save debug information
        use_main_cache: Use main pipeline cache namespace
        
    Returns:
        Test results dictionary
    """
    # Initialize test framework
    framework = StageTestFramework('2a', 'Soil K Extraction')
    framework.logger.info(f"Starting Stage 2A test for paper: {paper_filename}")
    
    try:
        # Load paper data
        framework.logger.info("Loading paper data...")
        paper_data = framework.load_paper_data(paper_filename)
        paper_id = paper_data.get('filename', 'unknown').replace('.json', '').replace('.pdf', '')
        
        # Display paper info
        print(f"\nğŸ“„ Testing Paper: {paper_id}")
        print(f"   Full text length: {len(paper_data.get('full_text', ''))} characters")
        print(f"   Tables available: {len(paper_data.get('table_data', []))}")
        
        # Initialize components
        framework.logger.info("Initializing Gemini client and components...")
        gemini_client = GeminiClient(api_key)
        prompt_loader = PromptLoader()
        
        # Create extractor
        extractor = SoilKExtractor(gemini_client, prompt_loader)
        
        # Check cache first (unless disabled)
        cache_paper_id = f"test_{paper_id}" if not use_main_cache else paper_id
        
        if not disable_cache:
            cache_source = "main pipeline" if use_main_cache else "isolated test"
            print(f"\nğŸ” Checking {cache_source} cache...")
            
            if framework.cache_manager.is_stage_cached(cache_paper_id, '2a'):
                framework.logger.info("Found cached results, loading...")
                cached_result = framework.cache_manager.get_cached_result(cache_paper_id, '2a')
                if cached_result and 'error' not in cached_result:
                    print(f"âœ… Using cached Stage 2A results from {cache_source}")
                    results = cached_result
                    # Estimate metrics from cached data
                    framework.update_metrics(
                        {"input": 0, "output": 0},  # No new tokens used
                        0.0  # No cost for cache hit
                    )
                else:
                    # Cache might have error, proceed with fresh processing
                    results = None
            else:
                results = None
        else:
            print("\nğŸ”„ Cache disabled, forcing fresh processing...")
            results = None
            
        # Process if no valid cached results
        if results is None:
            framework.logger.info("Processing Stage 2A soil K extraction...")
            framework.start_tracking()
            
            print("\nğŸ”„ Running Stage 2A: Soil K Specific Extraction...")
            print("   This extracts potassium-specific parameters, balances, and dynamics...")
            
            # Run extraction
            results = await extractor.extract(paper_data)
            
            # Get token usage from Gemini client
            if hasattr(gemini_client, 'last_token_count'):
                token_usage = {
                    "input": gemini_client.last_token_count.get('input_tokens', 0),
                    "output": gemini_client.last_token_count.get('output_tokens', 0),
                    "thinking": gemini_client.last_token_count.get('thinking_tokens', 0)
                }
            else:
                # Estimate based on text lengths
                token_usage = {
                    "input": len(str(paper_data)) // 4,  # Rough estimate
                    "output": len(str(results)) // 4,
                    "thinking": 0
                }
                
            # Calculate cost
            cost = calculate_stage_cost(
                token_usage['input'], 
                token_usage['output'],
                token_usage['thinking']
            )
            
            # Update metrics
            framework.update_metrics(token_usage, cost)
            
            # Cache the results (using appropriate cache namespace)
            if not disable_cache and 'error' not in results:
                cache_namespace = "main pipeline" if use_main_cache else "isolated test"
                framework.logger.info(f"Caching successful results to {cache_namespace}...")
                framework.cache_manager.cache_stage_result(cache_paper_id, '2a', results, paper_data)
                
        # Save output to standard pipeline location with test metadata
        output_path = framework.save_stage_output(paper_id, results, run_type="stage_test")
        
        # Validate results
        is_valid, validation_issues = framework.validate_stage_output(results)
        
        if not is_valid:
            framework.logger.warning(f"Validation issues found: {validation_issues}")
            print(f"\nâš ï¸  Validation Issues: {len(validation_issues)}")
            for issue in validation_issues:
                print(f"   - {issue}")
        else:
            print("\nâœ… Stage 2A output validation passed")
            
        # Display results
        framework.display_results(results, detailed=verbose)
        
        # Save debug information if requested
        if save_debug:
            debug_info = {
                "paper_id": paper_id,
                "paper_stats": {
                    "full_text_length": len(paper_data.get('full_text', '')),
                    "table_count": len(paper_data.get('table_data', [])),
                    "has_abstract": 'abstract' in paper_data.get('full_text', '').lower(),
                    "has_potassium_keywords": any(k in paper_data.get('full_text', '').lower() 
                                                for k in ['potassium', 'potash', 'k2o', 'kcl'])
                },
                "extraction_stats": {
                    "soil_k_parameters": len(results.get('soil_k_parameters', [])),
                    "k_balance_data": len(results.get('k_balance_measurements', [])),
                    "k_dynamics": len(results.get('k_dynamics', [])),
                    "agricultural_systems": len(results.get('agricultural_systems', [])),
                    "quantitative_k_data": len(results.get('quantitative_k_findings', []))
                },
                "validation": {
                    "is_valid": is_valid,
                    "issues": validation_issues
                }
            }
            
            debug_path = framework.stage_output_dir / f"{paper_id}_2a_debug.json"
            with open(debug_path, 'w', encoding='utf-8') as f:
                json.dump(debug_info, f, indent=2)
            framework.logger.info(f"Saved debug information to: {debug_path}")
            
        # Generate test summary
        test_result = {
            "success": is_valid and 'error' not in results,
            "paper_id": paper_id,
            "output_path": output_path,
            "validation": {
                "passed": is_valid,
                "issues": validation_issues
            },
            "metrics": {
                "processing_time": framework.start_time and (datetime.now().timestamp() - framework.start_time),
                "token_usage": framework.token_usage,
                "cost": framework.cost
            }
        }
        
        print(f"\nğŸ“ Results saved to standard pipeline: {output_path}")
        print(f"    Next stages will automatically find these results")
        
        return test_result
        
    except Exception as e:
        framework.logger.error(f"Stage 2A test failed: {str(e)}", exc_info=True)
        print(f"\nâŒ Stage 2A test failed: {str(e)}")
        
        # Save error information
        error_result = {
            "error": str(e),
            "stage": "2a",
            "paper_id": paper_filename,
            "timestamp": datetime.now().isoformat()
        }
        
        framework.save_stage_output(
            paper_filename.replace('.json', '').replace('.pdf', ''), 
            error_result,
            run_type="stage_test_error"
        )
        
        return {
            "success": False,
            "error": str(e),
            "paper_id": paper_filename
        }

def main():
    """Main entry point for Stage 2A testing"""
    # Initialize framework for argument parsing
    framework = StageTestFramework('2a', 'Soil K Extraction')
    
    # Get command line arguments
    parser = framework.get_standard_args(
        "Extract soil potassium specific parameters and dynamics from research papers"
    )
    
    # Add stage-specific arguments if needed
    parser.add_argument("--show-prompt", action="store_true",
                      help="Display the prompt being sent to Gemini")
    parser.add_argument("--use-main-cache", action="store_true",
                      help="Use main pipeline cache namespace (may find existing results)")
    parser.add_argument("--fresh-cache", action="store_true", 
                      help="Use isolated test cache namespace (default behavior)")
    
    args = parser.parse_args()
    
    # Validate paper exists
    if not validate_paper_exists(args.paper):
        print(f"âŒ Error: Paper '{args.paper}' not found in synthesis_ready directory")
        print("\nAvailable papers:")
        from stage_test_utils import load_paper_list
        papers = load_paper_list(limit=10)
        for i, paper in enumerate(papers[:10]):
            print(f"  {i+1}. {paper}")
        if len(papers) > 10:
            print(f"  ... and {len(papers) - 10} more")
        sys.exit(1)
        
    # Override output directory if specified
    if args.output_dir:
        framework.test_outputs_dir = args.output_dir
        framework.stage_output_dir = os.path.join(args.output_dir, f"stage_{framework.stage_id}")
        os.makedirs(framework.stage_output_dir, exist_ok=True)
        
    print(f"\n{'='*80}")
    print(f"STAGE 2A TEST: Soil K Specific Extraction")
    print(f"{'='*80}")
    
    # Show cache mode
    if args.use_main_cache:
        print("ğŸ”— Using main pipeline cache namespace (may find existing results)")
    else:
        print("ğŸ  Using isolated test cache namespace (fresh testing)")
        
    print("ğŸ“ Saving to standard pipeline outputs (8_stage_outputs/) for natural stage chaining")
    
    # Run the test
    result = asyncio.run(test_stage_2a(
        paper_filename=args.paper,
        api_key=args.api_key,
        disable_cache=args.disable_cache,
        verbose=args.verbose,
        save_debug=args.save_debug,
        use_main_cache=args.use_main_cache
    ))
    
    # Print final summary
    print(f"\n{'='*80}")
    if result['success']:
        print("âœ… Stage 2A test completed successfully")
    else:
        print("âŒ Stage 2A test failed")
        if 'error' in result:
            print(f"   Error: {result['error']}")
    print(f"{'='*80}")
    
    # Exit with appropriate code
    sys.exit(0 if result['success'] else 1)

if __name__ == "__main__":
    main()